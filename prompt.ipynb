{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linchihhsien/.local/share/virtualenvs/linchihhsien-pejDPtXM/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  pass\n"
     ]
    }
   ],
   "source": [
    "my_api_key = \"input your api key\"\n",
    "llm_from_openAI = ChatOpenAI(api_key = my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2396fd4b-0e5f-4f9a-b78e-c6177e3240f1-0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_from_openAI.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hugging Face's `transformers` library, OpenAI's `openai` library, and Cohere's `cohere` library offer LLMs.\" response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 131, 'total_tokens': 164}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4089a1c6-5b86-4fd6-8353-b56d69808da0-0'\n"
     ]
    }
   ],
   "source": [
    "response = llm_from_openAI.invoke([prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: What libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\" response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 131, 'total_tokens': 162}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a07f9f69-efdf-411d-b35c-5d4bae3f7571-0'\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")\n",
    "\n",
    "# 使用 invoke 方法生成响应\n",
    "response = llm_from_openAI.invoke([formatted_prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is like a box of chocolates, you never know what you're gonna get. But really, it's just a series of random events and experiences that we try to make sense of.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "# 设置 API 密钥\n",
    "my_api_key = \"api key\"\n",
    "\n",
    "# 创建 ChatOpenAI 实例，设置温度为 1.0 以增加创意性和随机性\n",
    "llm_from_openAI = ChatOpenAI(api_key=my_api_key, temperature=1.0)\n",
    "\n",
    "# 定义 prompt\n",
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "# 使用 invoke 方法生成响应\n",
    "response = llm_from_openAI.invoke([prompt])\n",
    "\n",
    "# 打印响应\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保險數學這門課中出席分數佔比為30%\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically concise and Accurate, producing\n",
    "accurate responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: 工資管系統計軟體與繪圖這門課中出席分數佔比多少？\n",
    "AI: 工資管系統計軟體與繪圖這門課中出席分數佔比為20%\n",
    "\n",
    "User: 工資管系計算機該論這門課中出席分數佔比多少？\n",
    "AI: 因為在我的資料庫中工資管系計算機該論這門課的課程大綱中並沒有描述出席分數的比重因此我無法回答你\n",
    "\n",
    "User: 生物統計這門課中出席分數佔比多少？\n",
    "課程背景：生物統計這門課中不包含出席分數\n",
    "AI: 這門課程中出席分數並不會影響總分\n",
    "\n",
    "User: 保險數學這門課中出席分數佔比多少？\n",
    "AI: \"\"\"\n",
    "\n",
    "response = llm_from_openAI.invoke([prompt])\n",
    "\n",
    "# 打印响应\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"工資管系管理學這門課中出席分數佔比多少？\",\n",
    "        \"answer\": \"根據課程大綱，出席分數通常佔整體成績的20%。\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"工資管系管理學的出席分數是多少？\",\n",
    "        \"answer\": \"根據我的知識，這門課程的出席分數佔比是20%。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: 工資管系管理學這門課中出席分數佔比多少？\n",
      "AI: 根據課程大綱，出席分數通常佔整體成績的20%。\n",
      "\n",
      "\n",
      "\n",
      "User: 工資管系管理學的出席分數是多少？\n",
      "AI: 根據我的知識，這門課程的出席分數佔比是20%。\n",
      "\n",
      "\n",
      "\n",
      "User: 工資管系管理學的出席分數是多少？\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"工資管系管理學的出席分數是多少？\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哇，你對出席分數真是關心啊！根據課程規劃，通常出席分數佔比是20%。記得好好去上課哦！\n"
     ]
    }
   ],
   "source": [
    "query = \"工資管系管理學的出席分數是多少？\"\n",
    "\n",
    "# 格式化提示\n",
    "formatted_prompt = few_shot_prompt_template.format(query=query)\n",
    "\n",
    "# 获取响应\n",
    "response = llm_from_openAI.invoke([formatted_prompt])\n",
    "\n",
    "# 打印响应内容\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
    "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite color?\",\n",
    "        \"answer\": \"79\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite food?\",\n",
    "        \"answer\": \"Carbon based lifeforms\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"What is the best thing in the world?\",\n",
    "        \"answer\": \"The perfect pizza.\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"If you could do anything in the world what would you do?\",\n",
    "        \"answer\": \"Take over the world, of course!\"\n",
    "    }, {\n",
    "        \"query\": \"Where should I travel?\",\n",
    "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
    "\n",
    "words = re.split('[\\n ]', some_text)\n",
    "print(words, len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They flap their wings and hope for the best. It's basically like winging it in life.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do birds fly?\"\n",
    "\n",
    "formatted_prompt = few_shot_prompt_template.format(query=query)\n",
    "\n",
    "# 获取响应\n",
    "response = llm_from_openAI.invoke([formatted_prompt])\n",
    "\n",
    "# 打印响应内容\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: What is the weather like today?\n",
      "AI: Cloudy with a chance of memes.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # increased max length\n",
    ")\n",
    "\n",
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義你的提示模板\n",
    "prompt = \"\"\"\n",
    "課程背景：管理學院中所有系所所開設課程中，出席情況通常會在一定的比例下影響總分。\n",
    "問題：工資管系統計軟體與繪圖這門課中出席分數佔比多少？\n",
    "\"\"\"\n",
    "\n",
    "# 使用提示模板來調用 AI 系統以獲取回應\n",
    "answer = retrieval_chain_OpenAI.invoke({'input': prompt})\n",
    "\n",
    "# 印出答案及任何額外的背景信息\n",
    "print(answer['answer'])\n",
    "for context in answer['context']:\n",
    "    print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically concise and Accurate, producing\n",
    "accurate responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: 工資管系統計軟體與繪圖這門課中出席分數佔比多少？\n",
    "AI: 工資管系統計軟體與繪圖這門課中出席分數佔比為20%\n",
    "\n",
    "User: 工資管系計算機該論這門課中出席分數佔比多少？\n",
    "AI: 因為在我的資料庫中工資管系計算機該論這門課的課程大綱中並沒有描述出席分數的比重因此我無法回答你\n",
    "\n",
    "User: 生物統計這門課中出席分數佔比多少？\n",
    "課程背景：生物統計這門課中不包含出席分數\n",
    "AI: 這門課程中出席分數並不會影響總分\n",
    "\n",
    "User: 保險數學這門課中出席分數佔比多少？\n",
    "AI: \"\"\"\n",
    "\n",
    "# 使用提示模板來調用 AI 系統以獲取回應\n",
    "answer = retrieval_chain_OpenAI.invoke({'input': prompt})\n",
    "\n",
    "# 印出答案及任何額外的背景信息\n",
    "print(answer['answer'])\n",
    "for context in answer['context']:\n",
    "    print(context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linchihhsien-pejDPtXM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
